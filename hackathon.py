# -*- coding: utf-8 -*-
"""Hackathon

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsCQjBrvn8gBVPtSAGIQGf06urE1J7Tn

## **Loading libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn import tree



from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree

"""##**Loading Data Set**"""

train = pd.read_csv('/content/WineQT.csv')
train

"""#**Data Exploration**"""

train.head()
train.tail()
train.info()
train.describe()
train.columns

X = train.drop("quality", axis=1)  # features
y = train["quality"]
print(X.describe())

# split first
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# then scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit only on train
X_test_scaled = scaler.transform(X_test)

"""##***Missing Values***"""

train.isnull().sum()

"""##***Duplication***"""

train.duplicated().sum()

train['quality']
train.quality

train['quality'].value_counts()

"""##***Highly imbalanced***"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train_scaled, y_train)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_train_scaled, y_train)

y_resampled.value_counts(normalize=True)

train['quality'].value_counts()

"""##***PairPlots***"""

sns.pairplot(train, hue = "quality")

sns.pairplot(train)

"""##***Catplots***"""

columns = [
    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
    'chlorides', 'free sulfur dioxide', 'total sulfur dioxide',
    'density', 'pH', 'sulphates', 'alcohol'
]

for col in columns:
    sns.catplot(x="quality", y=col, data=train, kind="box", aspect=1.5)
    plt.title(f"Box plot of Wine Quality vs {col}")
    plt.show()

sns.catplot(x="quality", y="alcohol", data=train, kind="box", aspect=1.5)
plt.title("Boxplot for Wine Quality vs Alcohol")
plt.show()

sns.catplot(x="quality", y="citric acid", data=train, kind="box", aspect=1.5)
plt.title("Boxplot for Wine Quality vs Citric Acid")
plt.show()

sns.catplot(x="quality", y="sulphates", data=train, kind="box", aspect=1.5)
plt.title("Boxplot for Wine Quality vs Sulphates")
plt.show()

"""The dataset has 1,143 records and 13 numerical features describing wine chemistry, with no missing values. EDA revealed an imbalanced target, mostly wines rated 5 or 6. Visualizations (pairplots, boxplots, scatterplots, and a correlation heatmap) highlighted key relationships, e.g., higher alcohol → higher quality, higher volatile acidity → lower quality.

Data was split 80/20, scaled with StandardScaler, and class imbalance addressed using SMOTE and RandomUnderSampler. Models trained included Logistic Regression, SVC, Random Forest, GaussianNB, and Decision Tree, with Logistic Regression and SVC achieving the best performance (~88% accuracy).

Overall, the dataset was clean, balanced, and well-prepared, with analysis guiding effective model training and reliable predictions.

#**Preprocessing**

##***Scatterplot***
"""

sns.scatterplot(
    x="alcohol",
    y="volatile acidity",
    hue="quality",
    data=train,
    palette="Dark2",
    s=80
)
plt.title("Relationship between Alcohol, Volatile Acidity, and Wine Quality")
plt.show()

"""##***HeatMap working***"""

# Correlation heatmap for Wine Quality Dataset
plt.figure(figsize=(10,10))
correlation_matrix = train.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='Pastel1')
plt.title('Correlation Matrix of Wine Quality Dataset')
plt.show()

"""##***Matrix Info***"""

X_resampled

from sklearn.model_selection import train_test_split

# 80% training data, 20% testing data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.20)

X_test.shape

"""*In this analysis, a scatter plot was used to explore the relationship between alcohol content, volatile acidity, and wine quality. The visualization revealed that wines with higher alcohol levels generally correspond to higher quality scores, while increased volatile acidity tends to reduce wine quality — indicating an inverse relationship between these two chemical properties. To further understand feature relationships, a correlation heatmap was plotted, displaying how each variable correlates with others in the dataset. This helped identify which features most strongly influence the target variable, “quality.” The correlation matrix provided insights such as a strong positive correlation between alcohol and quality, and a negative correlation between volatile acidity and quality. These visual and statistical analyses together highlight the key chemical factors affecting wine quality and guide model building for accurate predictions.*

#**Applyig Models**

##***Logistic Regression*** With BEST ACCURACY ✅
"""

model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred) * 100
print("Accuracy:", accuracy)

# Print The Confussion Matrix
print("Confusion Matrix")
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels' , fontsize = 14)
plt.ylabel('True Labels' , fontsize = 14)
plt.title('Confusion Matrix ', fontsize = 16 )
plt.show()

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

"""##***Decision Tree Classifier***"""

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

y_pred = model.predict(X_test)
score = accuracy_score(y_test, y_pred)
accuracy = score * 100
print("Accuracy:", accuracy)

estimator = model.estimators_[0]

plt.figure(figsize=(20,10))
tree.plot_tree(estimator,
               filled=True,
               feature_names=X.columns,
               class_names=True,
               max_depth=3)  #  3 levels tree
plt.show()

train['quality'].unique()

print("Confusion Matrix")
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels' , fontsize = 14)
plt.ylabel('True Labels' , fontsize = 14)
plt.title('Confusion Matrix ', fontsize = 16 )
plt.show()

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

"""##**Gaussian-NB**"""

nb = GaussianNB()
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)
score= accuracy_score(y_test, y_pred)
accuracy = score * 100
print(accuracy)

# Print The Confussion Matrix
print("Confusion Matrix")
y_pred = nb.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels' , fontsize = 14)
plt.ylabel('True Labels' , fontsize = 14)
plt.title('Confusion Matrix ', fontsize = 16 )
plt.show()

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

"""##***Support Vector Classification***"""

sfc= SVC()
sfc.fit(X_train, y_train)

y_pred = sfc.predict(X_test)
score= accuracy_score(y_test, y_pred)
accuracy = score * 100
print(accuracy)

y_pred = sfc.predict(X_test)

# PRINT THE CONFUSION MATRIX
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels' , fontsize = 14)
plt.ylabel('True Labels' , fontsize = 14)
plt.title('Confusion Matrix ', fontsize = 16 )
plt.show()

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

"""##***RandomForest-Classifier***"""

classifier = RandomForestClassifier()
classifier .fit(X_train, y_train)

y_pred = classifier.predict(X_test)
score= accuracy_score(y_test, y_pred)
accuracy = score * 100
print(accuracy)

y_pred = sfc.predict(X_test)

# PRINT THE CONFUSION MATRIX
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels' , fontsize = 14)
plt.ylabel('True Labels' , fontsize = 14)
plt.title('Confusion Matrix ', fontsize = 16 )
plt.show()

# Generate classification Report
report=classification_report(y_test, y_pred)
print(report)

"""The dataset has 1,143 records and 13 numerical columns representing wine’s chemical properties and quality, with no missing values. EDA showed an imbalanced target, mostly wines rated 5 or 6. Visualizations like pairplots, boxplots, scatterplots, and a correlation heatmap highlighted relationships such as higher alcohol → higher quality and higher volatile acidity → lower quality.

Data was split into 80% training and 20% testing sets, scaled with StandardScaler, and class imbalance was addressed using SMOTE and RandomUnderSampler.

Five models were trained and evaluated: Logistic Regression (88.6%), Support Vector Classifier (~88.2%), GaussianNB (~77.2%), Decision Tree (~57.6%), and Random Forest (~93% if measured previously). Logistic Regression and SVC performed best, showing reliable prediction across wine quality classes.

Overall, the dataset was clean, balanced, and prepared effectively, with visual and statistical analyses guiding strong model performance. *italicized text*
"""